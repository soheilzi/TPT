# TPT Project

Welcome to **TPT â€“ Thinkâ€‚â€¢â€‚Pruneâ€‚â€¢â€‚Train**! A framework for teaching large language models to solve math problems by learning from (and improving on) their own reasoning traces.

---

## ğŸš€ What is TPT?

TPT is a threeâ€‘step, iterative workflow:

1. **Think** â€“ The model generates multiple, detailed solution traces.2. **Prune** â€“ We automatically keep only the traces that reach the correct answer.3. **Train** â€“ The model fineâ€‘tunes on this highâ€‘quality synthetic data to boost its skills.

Loop the cycle â†’ watch the model level up. âœ¨

---

## ğŸ› ï¸ Workflow & Commands

Below is the minimal commandâ€‘line recipe for each stage. Adjust paths/flags to taste.

### 1. Think â€“ Generate Synthetic Traces (ğŸ’¡ `gen_synth.py`)

Produce *N* solution attempts per question.

```bash
python gen_synth.py \
  --model_name    google/gemma-2-2b-it \
  --max_model_len 1500 \
  --num_samples   5 \
  --math          data/gsm8ktrain.json \
  --output_dir    samples/math_train/2b
```

Outputs go to `samples/math_train/ft/e0.json â€¦ e5.json`.

### 2. Prune & Split (âœ‚ï¸ `evmath.py` â†’ ğŸ“„ `make_json.py`)

1. **Score correctness** with `evmath.py` (example):
   ```bash
   python evmath.py --samples_dir samples/math_train/ft --answer_path data gsm8ktrain --num_samples 5
   ```
   This writes `correct_answers.json` and `pass_at_k_results.json`.
2. **Create new train/eval JSON**:
   ```bash
   python make_json.py \
     --input        samples/math_train/correct_answers.json \
     --train_output data/next/train2k.json \
     --eval_output  data/next/evnext.json \
     --train_size   2000
   ```

Use the new data in the next TPT cycle (back to **Train**).

### 3. Train (ğŸš‚ `sft_math.py`)

Fineâ€‘tune the base model used to generate the data on the created dataset.

```bash
python sft_math.py \
  --model_name_or_path google/gemma-2-2b-it \
  --train_data_path data/next/train2k.json \
  --eval_data_path  data/next/evnext.json \
  --learning_rate   1e-6 \
  --output_dir      gemma-tpt
```

This produces a checkpoint under `gemma-tpt/` and logs to W&B (set your `project` and `name` inside the script).

---

## ğŸ“‚ Repository Structure

```
TPT/
â”œâ”€â”€ data/             # Datasets (initial + generated)
â”œâ”€â”€ gemma-tpt/        # Model checkpoints & artifacts
â”œâ”€â”€ samples/          # Synthetic traces
â”œâ”€â”€ wandb/            # Experiment tracking
â”œâ”€â”€ evmath.py         # Scoring / pruning script
â”œâ”€â”€ gen_eval.py       # Generates evaluation questions
â”œâ”€â”€ gen_synth.py      # Synthetic generation script (Think)
â”œâ”€â”€ make_json.py      # Builds new train/eval JSON (Prune)
â”œâ”€â”€ sft_math.py       # Supervised fineâ€‘tune (Train)
â”œâ”€â”€ README.md         # You are here
â”œâ”€â”€ requirements.txt  # Python deps
```

---

## âš™ï¸ Setup Guide

### Prerequisites

- PythonÂ 3.10
- `pip`

### Installation

```bash
git clone <repository-url>
cd <repository-folder>

# Create & activate venv
python3.10 -m venv tpt_env
source tpt_env/bin/activate   # Windows: tpt_env\Scripts\activate

# Install deps
python3.10 -m pip install -r requirements.txt

# Extra: flashinfer wheel (for vLLMâ€‘FlashAttention)
python3.10 -m pip install   https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.2/flashinfer-0.1.2+cu121torch2.3-cp310-cp310-linux_x86_64.whl
```

Activate later with:

```bash
source tpt_env/bin/activate   
```

---

Ready? Time to **Think â†’Â Prune â†’Â Train** and watch your model improve 
